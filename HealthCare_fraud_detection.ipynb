{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection By Using Open Source Medicare Data - part 1: pig + sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class author(object):\n",
    "    \n",
    "    name = \"Xinyu (Max) Liu\"\n",
    "\n",
    "    email = \"xinyulrsm@gmail.com\"\n",
    "\n",
    "    create =  \"01/16/2016\"\n",
    "\n",
    "    addreess = \"Waltham, MA 02453\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This topic is inspired by ref-1 and ref-2. Here I will show how do I use the same data resource by different techeniques for data preparation, feature engineering and model fitting. I divide this work to four parts:\n",
    "\n",
    "+ part-1: use hadoop + pig + sklearn ( this work )\n",
    "\n",
    "+ part-2: use hive + spark\n",
    "\n",
    "+ part-3: use hive + R\n",
    "\n",
    "+ part-4: use flink \n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "1) http://www.dataiku.com/blog/2015/08/12/Medicare_Fraud.html by Pierre Gutierrez @ Dataiku\n",
    "\n",
    "    Very interesting feature engineering. \n",
    "    \n",
    "2) http://nbviewer.jupyter.org/github/ofermend/IPython-notebooks/blob/master/blog-part-1.ipynb  by Ofer Mendelevitch \n",
    "\n",
    "    This is very good and \"classic\" tutorial for data science by using hadoop, pig, hive, spark and other techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 2013 Part D Prescriber data:\n",
    "\n",
    "https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Part-D-Prescriber.html\n",
    "\n",
    "2) 2013 payment data:\n",
    "\n",
    "https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads.html\n",
    "\n",
    "3) npi exclusions data:\n",
    "\n",
    "http://oig.hhs.gov/exclusions/exclusions_list.asp#instruct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/bin/hdfs dfs -mkdir -p /data/\n",
    "$HADOOP_HOME/bin/hdfs dfs -put /home/max/data/PARTD_PRESCRIBER_PUF_NPI_DRUG_13.tab    /data\n",
    "$HADOOP_HOME/bin/hdfs dfs -put /home/max/data/OP_DTL_GNRL_PGYR2013_P01152016.csv      /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pig script for data preparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash  \n",
    "pig dataPrep.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import zipfile \n",
    "\n",
    "from dbfread import DBF\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import ensemble \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import brier_score_loss, precision_score, recall_score,f1_score, roc_auc_score, accuracy_score \n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pydoop.hdfs as hdfs \n",
    "from pydoop.hdfs import path as hpath \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.charts import Bar\n",
    "from bokeh.io import output_notebook, hplot, vplot\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read fraud npi list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = \"/home/hduser/T4/data/healthCare/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd  /home/hduser/T4/data/healthCare/\n",
    "ls *.DBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read all the npis in the DBF files\n",
    "npis = []\n",
    "for fnx in os.listdir(dataDir):\n",
    "    if fnx.endswith('EXCL.DBF'):\n",
    "        fn = os.path.join(dataDir, fnx)\n",
    "        for record in DBF(fn):\n",
    "            #print(record)\n",
    "            npi = record['NPI']\n",
    "            if npi != '0000000000':\n",
    "                #print npi\n",
    "                npis.append(int(npi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a DataFrame for 'is_fraud' npis\n",
    "df_npi_fraud = pd.DataFrame([[npi, 1.0] for npi in npis], columns=['npi', 'is_fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_npi_fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Pard_D_13 data from hdfs generated by pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read files from HDFS\n",
    "# Note that the results generated by pig script is a folder instead of a file\n",
    "# we need need to conbine all the parts in the folder to a pandas DataFrame\n",
    "\n",
    "def hdfs_read_csv_from_hdfs(path, sep=',', cols=None, col_types=None):\n",
    "    files = hdfs.ls(path);\n",
    "    file_parts = []\n",
    "\n",
    "    for f in files:\n",
    "        print f\n",
    "        print os.path.basename(f)\n",
    "        if os.path.basename(f).startswith('_'):\n",
    "            continue\n",
    "        with hdfs.open(f) as fh:\n",
    "            file_parts.append(pd.read_csv(fh, sep=sep, header=None, error_bad_lines=False))\n",
    "\n",
    "    df = pd.concat(file_parts, ignore_index=True)\n",
    "    if cols is not  None:\n",
    "        if len(cols) == len(df.columns):\n",
    "            df.columns = cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols1 = ['npi']\n",
    "cols2 = ['count','specialty']\n",
    "cols3 = ['last_name' , 'first_name', 'city', 'state']\n",
    "cols4 = ['claim_min','claim_max','claim_sum','supply_min','supply_max','supply_sum','drug_min',\n",
    "'drug_max','drug_sum']\n",
    "cols5 = ['last_name_1' , 'first_name_1', 'city_1', 'state_1']\n",
    "cols6 = [\"payment_count\", \"total_payment\"]\n",
    "\n",
    "cols = cols1 + cols2 + cols3 + cols4 + cols5 +cols6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/data/HealthCare/npi_payment_drugs_all.csv'\n",
    "\n",
    "df_partD = hdfs_read_csv_from_hdfs(data_path, sep='\\t', cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop 'last_name' , 'first_name', 'city', and 'state' from DataFrame\n",
    "# we don't need them in the model\n",
    "df_partD.drop(cols3 + cols5,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_partD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPI and drug group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_npi_drug = [\"npi\", \"drug\", \"count\", \"total_claim_count\", \"total_day_supply\", \"total_drug_cost\"]\n",
    "\n",
    "# the file below is generated by pig\n",
    "data_path = '/data/HealthCare/partd_13_npi_drug_all.csv'\n",
    "\n",
    "npi_drug = hdfs_read_csv_from_hdfs(data_path, sep='\\t', cols=cols_npi_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npi_drug['total_claim_count'] = npi_drug['total_claim_count'].map(lambda x: np.log10(x + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npi_drug['total_day_supply'] = npi_drug['total_day_supply'].map(lambda x: np.log10(x + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npi_drug['total_drug_cost'] = npi_drug['total_drug_cost'].map(lambda x: np.log10(x + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npi_drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(npi_drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Pard_D data and is_fraud data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_partD, df_npi_fraud, how='left', on='npi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df_partD), len(df_partD.columns)\n",
    "print len(df), len(df.columns)\n",
    "del(df_partD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill nans by 0\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['total_payment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we found 427 record we could use as fraud\n",
    "len(df[df['is_fraud']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add three more features\n",
    "df['claim_max-min'] = df['claim_max']-df['claim_min']\n",
    "df['supply_max-min'] = df['supply_max']-df['supply_min']\n",
    "df['drug_max-min'] = df['drug_max']-df['drug_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data to training set and validation set¶\n",
    "\n",
    "### we weill divide the data to training set and validation set. We will only use training set to select feature to avoid data leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO add code for five-fold cross-validation\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n",
    "ix_ran = df.index.values\n",
    "random.shuffle(ix_ran)\n",
    "\n",
    "df_len = len(df)\n",
    "train_len = int(df_len * 0.8)  # 80% for training\n",
    "\n",
    "#kf = KFold(df_len, n_folds=5)\n",
    "#for ix_train, ix_valid in kf:\n",
    "\n",
    "ix_train = ix_ran[:train_len]\n",
    "ix_valid = ix_ran[train_len:]\n",
    "\n",
    "df_train = df.ix[ix_train]\n",
    "df_valid = df.ix[ix_valid]\n",
    "\n",
    "print  len(ix_train), len(ix_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npi_drug_w_flag_train= pd.merge(npi_drug, df_train[['npi','is_fraud']], how='inner', on=['npi'])\n",
    "\n",
    "npi_drug_w_flag_all= pd.merge(npi_drug, df[['npi','is_fraud']], how='inner', on=['npi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(npi_drug_w_flag_train[npi_drug_w_flag_train['is_fraud']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['total_claim_count' , 'total_day_supply' , 'total_drug_cost' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique drug names\n",
    "drugs = set([ drugx for drugx in npi_drug_w_flag_train['drug'].values if isinstance(drugx, str)])\n",
    "print len(drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Total records in npi_drug train set : \", len(npi_drug_w_flag_train)\n",
    "print \"is_fraud recodes :  \", len(npi_drug_w_flag_train[npi_drug_w_flag_train['is_fraud']==1])\n",
    "npi_drug_w_flag_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ng_train = npi_drug_w_flag_train.groupby(['drug', 'is_fraud'])\n",
    "ng_all = npi_drug_w_flag_all.groupby(['drug', 'is_fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngkeys = ng_train.groups.keys()\n",
    "print len(ngkeys)\n",
    "print ngkeys[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drug_has_both_01 = [drugx for drugx in drugs if ((drugx,0.0) in ngkeys ) & ( (drugx,1.0) in ngkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO use agg to replace the code below\n",
    "re_drug_tt = dict()\n",
    "for drugx in drug_has_both_01:\n",
    "    for colx in cols:\n",
    "        fraud_0 = ng_train.get_group((drugx,0.0))[colx].values\n",
    "        fraud_1 = ng_train.get_group((drugx,1.0))[colx].values\n",
    "        # print len(fraud_0), len(fraud_1)\n",
    "        if (len(fraud_0)>2) & (len(fraud_1)>2) :\n",
    "            tt = ttest_ind(fraud_0, fraud_1)\n",
    "            re_drug_tt[(drugx, colx)] = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p005 = [(key, p) for (key, (t, p)) in re_drug_tt.items() if p <=0.05]  # p = 0.1 or 0.05\n",
    "print len(p005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import BoxPlot,Histogram, output_file, show, hplot,vplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm=100\n",
    "drug_name = p005[mm][0][0]\n",
    "print drug_name\n",
    "df_bar = pd.concat([ng_train.get_group((p005[mm][0][0],0.0)), ng_train.get_group((p005[mm][0][0],1.0))])\n",
    "df_bar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "box = [\n",
    "    BoxPlot(df_bar, values=col, label='is_fraud', title= \"p-value: \" + \"%0.2e\"%(re_drug_tt[(drug_name, col)][1]), \n",
    "               color='is_fraud', plot_width=300, plot_height=500)\n",
    "       for col in [\"total_claim_count\", \"total_day_supply\", \"total_drug_cost\"]\n",
    "      ]\n",
    "#output_file(drug_name + '.html')\n",
    "print drug_name\n",
    "show(hplot(*box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = [Histogram(df_bar, values=col, label='is_fraud',  color='is_fraud')\n",
    "        for col  in [\"total_claim_count\", \"total_day_supply\", \"total_drug_cost\"]\n",
    "      ]\n",
    "show(vplot(*hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuer engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Drug score\n",
    "### select drug and catagory from T-test by p value\n",
    "\n",
    "### logistic regreassion to for a \"drug score\" calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ng_list = []\n",
    "new_col_all =[]\n",
    "for i, p005x in enumerate(p005):\n",
    "    #if i>4:\n",
    "    #   break\n",
    "    drug_name = p005x[0][0]\n",
    "    cat_name = p005x[0][1] \n",
    "    \n",
    "    new_col = drug_name+'_'+cat_name\n",
    "    new_col_all.append(new_col)\n",
    "\n",
    "    ng_0 = ng_all.get_group((drug_name,0.0))[['npi', cat_name]]\n",
    "    ng_1 = ng_all.get_group((drug_name,1.0))[['npi', cat_name]]\n",
    "\n",
    "    ng_01 = pd.concat([ng_0, ng_1])\n",
    "    ng_01.rename(columns={cat_name: new_col}, inplace=True)\n",
    "    ng_list.append(ng_01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npi_col = df[['npi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_npi = []\n",
    "\n",
    "for n, nx in enumerate(ng_list):\n",
    "    \n",
    "    nggx = pd.merge(npi_col, nx.drop_duplicates(['npi']), on='npi', how='left')\n",
    "    #print n, len(nggx), len(npi_col)\n",
    "    w_npi.append(nggx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for wx in w_npi:\n",
    "    col_n = wx.columns[1]\n",
    "    df[col_n] = wx[col_n].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wx = w_npi[0]\n",
    "wx.columns[1]\n",
    "col_n = wx.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wx[col_n].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del(wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.ix[ix_train]\n",
    "df_valid = df.ix[ix_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the code below will cause a memory issue, why?\n",
    "\n",
    "#for ng_list_x in ng_list:\n",
    "#    df=pd.merge(df, ng_list_x, on='npi', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X= df_train[new_col_all].values\n",
    "Y = df_train['is_fraud'].values\n",
    "clf =  LogisticRegression(C=1e5, class_weight={0:1, 1:4000}, n_jobs=3)\n",
    "clf.fit(X,Y)\n",
    "y_p=clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df[new_col_all].values\n",
    "y_p = clf.predict_proba(X)\n",
    "df['drug_score'] = y_p[:,1]\n",
    "\n",
    "df_train = df.ix[ix_train]\n",
    "df_valid = df.ix[ix_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speciaty score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_dict =[]\n",
    "spec_fraud_1 = df_train[df_train['is_fraud']==1]['specialty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(spec_fraud_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_dict =  dict(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_dict.get('343',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['spec_score'] = df['specialty'].map(lambda x: spec_dict.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df.ix[ix_train]\n",
    "df_valid = df.ix[ix_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerical_feas_sel = ['count',\n",
    "                     'claim_min','claim_max', 'claim_sum',\n",
    "                     'supply_min','supply_max','supply_sum',\n",
    "                     'drug_min','drug_max','drug_sum',\n",
    "                     'spec_score','drug_score',\n",
    "                     'payment_count', 'total_payment']\n",
    "# 'claim_max-min','supply_max-min','drug_max-min', \n",
    "target = 'is_fraud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_0 = {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 1, 'learning_rate': 0.01}\n",
    "params_1 = {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 1, 'class_weight' : {0:1, 1:4000}, 'n_jobs':3}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "clfs = [\n",
    "    LogisticRegression(C=1e5,class_weight={0:1, 1:4000}, n_jobs=3),\n",
    "    \n",
    "    GaussianNB(),\n",
    "\n",
    "    ensemble.RandomForestClassifier(**params_1),\n",
    "\n",
    "    ensemble.ExtraTreesClassifier(**params_1),\n",
    "    \n",
    "    ensemble.GradientBoostingClassifier(**params_0)\n",
    "    \n",
    "    ]\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = df_train[numerical_feas_sel].values\n",
    "\n",
    "y_train = df_train['is_fraud'].values\n",
    "    \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_valid = df_valid[numerical_feas_sel].values\n",
    "y_valid = df_valid['is_fraud'].values\n",
    "X_valid_x= scaler.transform(X_valid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_result = []\n",
    "df_m = []\n",
    "clfs_fited = []\n",
    "for clf in clfs:\n",
    "    print(\"%s:\" %  clf.__class__.__name__)\n",
    "    clf.fit(X_train,y_train)\n",
    "    clfs_fited.append(clf)\n",
    "    y_pred = clf.predict(X_valid_x)\n",
    "    prob_pos  = clf.predict_proba(X_valid_x)[:, 1]\n",
    "    prob_result.append(prob_pos)\n",
    "    m = confusion_matrix(y_valid, y_pred)\n",
    "    clf_score = brier_score_loss(y_valid, prob_pos, pos_label=y_valid.max())\n",
    "    print(\"\\tBrier: %1.5f\" % (clf_score))\n",
    "    print(\"\\tPrecision: %1.5f\" % precision_score(y_valid, y_pred))\n",
    "    print(\"\\tRecall: %1.5f\" % recall_score(y_valid, y_pred))\n",
    "    print(\"\\tF1: %1.5f\" % f1_score(y_valid, y_pred))\n",
    "    print(\"\\tauc: %1.5f\" % roc_auc_score(y_valid, prob_pos))\n",
    "    print(\"\\tAccuracy: %1.5f\\n\" % accuracy_score(y_valid, y_pred))\n",
    "    df_m.append(\n",
    "        pd.DataFrame(m, index=['True Negative', 'True Positive'], columns=['Pred. Negative', 'Pred. Positive'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_valid, prob_result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TOOLS = 'pan, wheel_zoom,box_zoom,box_select,crosshair,resize,reset, hover'\n",
    "p = figure(tools=TOOLS)\n",
    "p.circle(fpr,tpr, size=4)\n",
    "p.title = \"ROC\"\n",
    "p.xaxis.axis_label  = \"FP rate\"\n",
    "p.yaxis.axis_label  = \"TP rate\"\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = clfs_fited[2].feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerical_feas_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance[sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feas = [numerical_feas_sel[ix] for ix in sorted_idx]\n",
    "bardata = {\"name\":feas[::-1], \"importance percent\":feature_importance[sorted_idx][::-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Bar, output_file, show, hplot\n",
    "from bokeh.charts.attributes import ColorAttr, CatAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bar = Bar(bardata, values=\"importance percent\", label=CatAttr(columns=[\"name\"], sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
